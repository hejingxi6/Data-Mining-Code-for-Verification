{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0dbbfdce-2885-4c72-8045-6b447d22268d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“‚ Data file: C:\\Users\\hejin\\DM_FINALVERSION_ENG\\Book1_5x_noise20-3.csv\n",
      "ğŸ“¥ Output dir: C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\n",
      "\n",
      "âœ… Loaded: 955 rows, 168 columns\n",
      "ğŸ§¹ Dropped columns with â‰¥95% missing: 1\n",
      "ğŸ¯ Features: 1600 | Samples: 955\n",
      "âœ‚ï¸ No saved split found; created a fresh 90/10 split\n",
      "ğŸ“Š Train: 859 | Test: 96\n",
      "\n",
      "ğŸš€ Training Gradient Boosting (same as main)â€¦\n",
      "â±ï¸ Fit time: 14.71s\n",
      "ğŸ† Test: RÂ²=0.955 | RMSE=5.99 | MAE=3.92\n",
      "\n",
      "[A] Training quantile GB to estimate 90% prediction intervals\n",
      "\n",
      "[B] Repeated K-Fold CV + validation curve\n",
      "\n",
      "[C] Error by quantile buckets\n",
      "\n",
      "[D] Permutation Importance (PI) + stability\n",
      "\n",
      "[E] PDP/ICE/2D interactions\n",
      "\n",
      "[F] PSI: Train vs Test distribution difference (top numeric features)\n",
      "\n",
      "âœ… Enhanced analysis completed. Artifacts:\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\GB_pred_detail.csv\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\GB_PI_coverage.csv\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\B1_CV_distribution.csv\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\B1_CV_violin.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\B2_validation_curve_n_estimators.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\C1_bucket_by_pred.csv\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\C2_bucket_by_true.csv\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\C1_MAE_by_pred_quantile.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\C2_MAE_by_true_quantile.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\D1_PI_test.csv\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\D1_PI_barh.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\D2_PI_rank_stability.csv\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\D2_PI_rank_stability.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\E1_PDP_ICE.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\E2_PDP_2D.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\F1_PSI_train_vs_test.csv\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\F1_PSI_barh.png\n",
      " - C:\\Users\\hejin\\DM_FINALVERSION_ENG\\æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\\A1_pred_with_PI.png\n",
      "\n",
      "â±ï¸ Total time: 1132.88s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 600x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Enhanced Analysis (same directory, same random seed, same 90/10 split as the main experiment)\n",
    "Outputs:\n",
    " - Uncertainty quantification: quantile interval coverage + prediction band figure\n",
    " - Generalization robustness: repeated K-fold distribution + validation curve\n",
    " - Error profiling: MAE by predicted/true quantiles\n",
    " - Interpretability: Permutation Importance + stability + PDP/ICE + 2D interactions\n",
    " - Data shift: PSI (Train vs Test) on top numeric features\n",
    " - Reusable tables: prediction details, error buckets, PI, PSI, etc. (CSV)\n",
    "\"\"\"\n",
    "\n",
    "import os, json, time, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import RepeatedKFold, validation_curve, train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.inspection import PartialDependenceDisplay, permutation_importance\n",
    "\n",
    "# -------------------- Global plotting / fonts --------------------\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams[\"axes.unicode_minus\"] = False\n",
    "warnings.filterwarnings(\"ignore\", message=r\"Glyph \\d+ .* missing from font\", category=UserWarning)\n",
    "\n",
    "import matplotlib.font_manager as fm\n",
    "FONT_CHAIN = [\n",
    "    \"Microsoft YaHei\",\"Noto Sans CJK SC\",\"SimHei\",\"PingFang SC\",\"Heiti SC\",\n",
    "    \"Arial Unicode MS\",\"DejaVu Sans\"\n",
    "]\n",
    "plt.rcParams[\"font.family\"] = \"sans-serif\"\n",
    "plt.rcParams[\"font.sans-serif\"] = FONT_CHAIN\n",
    "\n",
    "# -------------------- Paths / constants --------------------\n",
    "ROOT = Path.cwd()\n",
    "REPORT_DIR = ROOT / \"æŠ¥å‘Šç¬¬ä¸€ç‰ˆ\"  # keep path unchanged as requested\n",
    "REPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATA_PATH = ROOT / \"Book1_5x_noise20-3.csv\"  # keep the same as main experiment; change here if needed\n",
    "TARGET = \"average_renewable_energy_usage_percent\"\n",
    "RS = 42\n",
    "\n",
    "print(f\"\\nğŸ“‚ Data file: {DATA_PATH}\")\n",
    "print(f\"ğŸ“¥ Output dir: {REPORT_DIR}\\n\")\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "# -------------------- Read & preprocess (aligned with main experiment) --------------------\n",
    "if not DATA_PATH.exists():\n",
    "    raise FileNotFoundError(f\"Data file not found: {DATA_PATH}\")\n",
    "\n",
    "df_raw = pd.read_csv(DATA_PATH)\n",
    "print(f\"âœ… Loaded: {df_raw.shape[0]} rows, {df_raw.shape[1]} columns\")\n",
    "\n",
    "# Drop extremely sparse columns\n",
    "missing_ratio = df_raw.isna().mean(numeric_only=False)\n",
    "high_missing_cols = missing_ratio[missing_ratio >= 0.95].index.tolist()\n",
    "if high_missing_cols:\n",
    "    df_raw = df_raw.drop(columns=high_missing_cols)\n",
    "    print(f\"ğŸ§¹ Dropped columns with â‰¥95% missing: {len(high_missing_cols)}\")\n",
    "\n",
    "# One-Hot encoding\n",
    "obj_cols = df_raw.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "if TARGET not in df_raw.columns:\n",
    "    raise ValueError(f\"âŒ Target column {TARGET} not found in raw data\")\n",
    "y_raw = pd.to_numeric(df_raw[TARGET], errors=\"coerce\")\n",
    "if y_raw.isna().any():\n",
    "    y_raw = y_raw.fillna(np.nanmedian(y_raw))\n",
    "\n",
    "df_oh = pd.get_dummies(df_raw, columns=obj_cols, drop_first=True)\n",
    "y = pd.to_numeric(df_oh[TARGET], errors=\"coerce\").fillna(np.nanmedian(y_raw))\n",
    "X = df_oh.drop(columns=[TARGET]).apply(pd.to_numeric, errors=\"coerce\")\n",
    "X = X.fillna(X.median(numeric_only=True))\n",
    "all_nan_cols = X.columns[X.isna().all()].tolist()\n",
    "if all_nan_cols:\n",
    "    X = X.drop(columns=all_nan_cols)\n",
    "\n",
    "print(f\"ğŸ¯ Features: {X.shape[1]} | Samples: {len(X)}\")\n",
    "\n",
    "# -------------------- Split (reuse saved indices from main experiment if present) --------------------\n",
    "split_file = REPORT_DIR / f\"split_indices_90_10_rs{RS}.npz\"\n",
    "if split_file.exists():\n",
    "    test_idx = np.load(split_file)[\"test_idx\"]\n",
    "    test_idx = pd.Index(test_idx)\n",
    "    all_idx = X.index\n",
    "    train_idx = all_idx.difference(test_idx)\n",
    "    X_train, X_test = X.loc[train_idx], X.loc[test_idx]\n",
    "    y_train, y_test = y.loc[train_idx], y.loc[test_idx]\n",
    "    print(\"ğŸ” Reused main-experiment split (90/10, same seed)\")\n",
    "else:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.10, random_state=RS, shuffle=True\n",
    "    )\n",
    "    print(\"âœ‚ï¸ No saved split found; created a fresh 90/10 split\")\n",
    "\n",
    "print(f\"ğŸ“Š Train: {len(X_train)} | Test: {len(X_test)}\")\n",
    "\n",
    "# -------------------- Train the same GB model --------------------\n",
    "base_params = dict(n_estimators=500, learning_rate=0.05, max_depth=4, random_state=RS)\n",
    "model = GradientBoostingRegressor(**base_params)\n",
    "print(\"\\nğŸš€ Training Gradient Boosting (same as main)â€¦\")\n",
    "t1 = time.time()\n",
    "model.fit(X_train, y_train)\n",
    "print(f\"â±ï¸ Fit time: {time.time()-t1:.2f}s\")\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "R2  = r2_score(y_test, y_pred)\n",
    "RMSE = float(np.sqrt(mean_squared_error(y_test, y_pred)))\n",
    "MAE  = float(mean_absolute_error(y_test, y_pred))\n",
    "print(f\"ğŸ† Test: RÂ²={R2:.3f} | RMSE={RMSE:.2f} | MAE={MAE:.2f}\")\n",
    "\n",
    "# Save prediction details\n",
    "pred_df = pd.DataFrame({\n",
    "    \"y_true\": y_test.values,\n",
    "    \"y_pred\": y_pred,\n",
    "    \"error\": y_test.values - y_pred,\n",
    "    \"abs_error\": np.abs(y_test.values - y_pred),\n",
    "}, index=y_test.index)\n",
    "pred_path = REPORT_DIR / \"GB_pred_detail.csv\"\n",
    "pred_df.to_csv(pred_path, index=True, encoding=\"utf-8-sig\")\n",
    "\n",
    "# -------------------- A. Uncertainty via quantile GB (robust error bars) --------------------\n",
    "print(\"\\n[A] Training quantile GB to estimate 90% prediction intervals\")\n",
    "gb_lower = GradientBoostingRegressor(loss=\"quantile\", alpha=0.05, **base_params)\n",
    "gb_upper = GradientBoostingRegressor(loss=\"quantile\", alpha=0.95, **base_params)\n",
    "gb_lower.fit(X_train, y_train)\n",
    "gb_upper.fit(X_train, y_train)\n",
    "y_lo = gb_lower.predict(X_test)\n",
    "y_hi = gb_upper.predict(X_test)\n",
    "\n",
    "# Fix crossing + non-negative yerr + recompute coverage\n",
    "ylo = np.minimum(y_lo, y_hi)\n",
    "yhi = np.maximum(y_lo, y_hi)\n",
    "coverage = float(np.mean((y_test.to_numpy() >= ylo) & (y_test.to_numpy() <= yhi)))\n",
    "band_width = float(np.mean(yhi - ylo))\n",
    "\n",
    "lower_err = np.maximum(0.0, (y_pred - ylo))\n",
    "upper_err = np.maximum(0.0, (yhi - y_pred))\n",
    "lower_err = np.nan_to_num(lower_err, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "upper_err = np.nan_to_num(upper_err, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "plt.figure(figsize=(9, 8))\n",
    "plt.scatter(y_test, y_pred, alpha=0.7, label=\"Prediction\")\n",
    "lo_axis = float(min(y_test.min(), y_pred.min()))\n",
    "hi_axis = float(max(y_test.max(), y_pred.max()))\n",
    "plt.plot([lo_axis, hi_axis], [lo_axis, hi_axis], \"r--\", lw=2, label=\"y = x\")\n",
    "\n",
    "yerr = np.vstack([lower_err, upper_err])  # (2, N) asymmetric errors\n",
    "plt.errorbar(\n",
    "    y_test.to_numpy(), y_pred, yerr=yerr,\n",
    "    fmt=\"none\", alpha=0.25, capsize=2, errorevery=2, label=\"90% interval\"\n",
    ")\n",
    "\n",
    "plt.title(f\"Prediction vs. 90% Interval (coverage={coverage:.2%}, avg width={band_width:.2f})\")\n",
    "plt.xlabel(\"Actual\"); plt.ylabel(\"Predicted\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR / \"A1_pred_with_PI.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "pd.DataFrame([{\"PI_coverage@90%\": coverage, \"avg_band_width\": band_width}]).to_csv(\n",
    "    REPORT_DIR / \"GB_PI_coverage.csv\", index=False, encoding=\"utf-8-sig\"\n",
    ")\n",
    "\n",
    "# -------------------- B. Generalization: CV distribution & validation curve --------------------\n",
    "print(\"\\n[B] Repeated K-Fold CV + validation curve\")\n",
    "rkf = RepeatedKFold(n_splits=5, n_repeats=5, random_state=RS)\n",
    "r2_list, rmse_list, mae_list = [], [], []\n",
    "for tr_idx, va_idx in rkf.split(X, y):\n",
    "    Xtr, Xva = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "    ytr, yva = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "    m = GradientBoostingRegressor(**base_params).fit(Xtr, ytr)\n",
    "    yp = m.predict(Xva)\n",
    "    r2_list.append(r2_score(yva, yp))\n",
    "    rmse_list.append(np.sqrt(mean_squared_error(yva, yp)))\n",
    "    mae_list.append(mean_absolute_error(yva, yp))\n",
    "\n",
    "cv_df = pd.DataFrame({\"R2\": r2_list, \"RMSE\": rmse_list, \"MAE\": mae_list})\n",
    "cv_df.to_csv(REPORT_DIR/\"B1_CV_distribution.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "sns.violinplot(data=cv_df[[\"R2\",\"RMSE\",\"MAE\"]], inner=\"quartile\")\n",
    "plt.title(\"Repeated K-Fold (5Ã—5) Performance Distribution\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR/\"B1_CV_violin.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "param_range = np.array([100, 200, 300, 500, 800])\n",
    "train_scores, valid_scores = validation_curve(\n",
    "    GradientBoostingRegressor(learning_rate=0.05, max_depth=4, random_state=RS),\n",
    "    X, y, param_name=\"n_estimators\", param_range=param_range, scoring=\"r2\", cv=5\n",
    ")\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(param_range, train_scores.mean(axis=1), \"o-\", label=\"Train RÂ²\")\n",
    "plt.plot(param_range, valid_scores.mean(axis=1), \"o-\", label=\"Validation RÂ²\")\n",
    "plt.fill_between(param_range, valid_scores.mean(1)-valid_scores.std(1),\n",
    "                 valid_scores.mean(1)+valid_scores.std(1), alpha=0.2)\n",
    "plt.xlabel(\"n_estimators\"); plt.ylabel(\"RÂ²\"); plt.title(\"Validation Curve: n_estimators\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR/\"B2_validation_curve_n_estimators.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# -------------------- C. Error profiling: quantile buckets --------------------\n",
    "print(\"\\n[C] Error by quantile buckets\")\n",
    "def bucket_mae(series_ref, errors, q=10, label=\"pred\"):\n",
    "    qcuts = pd.qcut(series_ref, q=q, duplicates=\"drop\")\n",
    "    # key: observed=True to silence future pandas warning and keep current behavior\n",
    "    grp = (pd.DataFrame({\"bucket\": qcuts, \"abs_error\": np.abs(errors)})\n",
    "           .groupby(\"bucket\", observed=True)[\"abs_error\"])\n",
    "    out = grp.agg([\"count\",\"mean\"]).reset_index().rename(columns={\"mean\": f\"MAE_by_{label}\"})\n",
    "    return out\n",
    "\n",
    "err = y_test.values - y_pred\n",
    "b1 = bucket_mae(pd.Series(y_pred, index=y_test.index), err, q=10, label=\"pred\")\n",
    "b2 = bucket_mae(pd.Series(y_test.values, index=y_test.index), err, q=10, label=\"true\")\n",
    "b1.to_csv(REPORT_DIR/\"C1_bucket_by_pred.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "b2.to_csv(REPORT_DIR/\"C2_bucket_by_true.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(len(b1))+1, b1[\"MAE_by_pred\"], \"o-\")\n",
    "plt.title(\"MAE by Predicted Quantiles (harder to the right)\")\n",
    "plt.xlabel(\"Predicted quantile bucket (1 = lowest)\"); plt.ylabel(\"MAE\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR/\"C1_MAE_by_pred_quantile.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(np.arange(len(b2))+1, b2[\"MAE_by_true\"], \"o-\")\n",
    "plt.title(\"MAE by True Quantiles\")\n",
    "plt.xlabel(\"True quantile bucket (1 = lowest)\"); plt.ylabel(\"MAE\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR/\"C2_MAE_by_true_quantile.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# -------------------- D. Interpretability: PI (+ stability) --------------------\n",
    "print(\"\\n[D] Permutation Importance (PI) + stability\")\n",
    "pi = permutation_importance(model, X_test, y_test, n_repeats=10, scoring=\"r2\", random_state=RS)\n",
    "pi_df = pd.DataFrame({\n",
    "    \"Feature\": X_test.columns,\n",
    "    \"PI_mean\": pi.importances_mean,\n",
    "    \"PI_std\": pi.importances_std\n",
    "}).sort_values(\"PI_mean\", ascending=False)\n",
    "pi_df.to_csv(REPORT_DIR/\"D1_PI_test.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "topk = min(15, len(pi_df))\n",
    "top_df = pi_df.head(topk).iloc[::-1]\n",
    "plt.figure(figsize=(max(11, 0.45*topk+9), 7))\n",
    "plt.barh(top_df[\"Feature\"], top_df[\"PI_mean\"], xerr=top_df[\"PI_std\"], alpha=0.8)\n",
    "plt.title(f\"Permutation Importance (Top {topk}, test set)\")\n",
    "plt.xlabel(\"importance (mean Â± std over permutations)\"); plt.ylabel(\"Feature\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR/\"D1_PI_barh.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# Stability: bootstrap ranking fluctuation\n",
    "boot = 15\n",
    "ranks = []\n",
    "rng = np.random.default_rng(RS)\n",
    "test_idx_arr = X_test.index.to_numpy()\n",
    "for b in range(boot):\n",
    "    sub_idx = rng.choice(test_idx_arr, size=len(test_idx_arr), replace=True)\n",
    "    res = permutation_importance(model, X_test.loc[sub_idx], y_test.loc[sub_idx],\n",
    "                                 n_repeats=5, scoring=\"r2\", random_state=RS+b)\n",
    "    mean_imp = pd.Series(res.importances_mean, index=X_test.columns).sort_values(ascending=False)\n",
    "    ranks.append(mean_imp.rank(ascending=False, method=\"min\"))\n",
    "rank_df = pd.concat(ranks, axis=1)\n",
    "rank_stats = pd.DataFrame({\n",
    "    \"Feature\": rank_df.index,\n",
    "    \"rank_mean\": rank_df.mean(1),\n",
    "    \"rank_std\": rank_df.std(1)\n",
    "}).sort_values(\"rank_mean\").head(topk)\n",
    "rank_stats.to_csv(REPORT_DIR/\"D2_PI_rank_stability.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "plt.figure(figsize=(max(11, 0.45*topk+9), 7))\n",
    "plt.errorbar(rank_stats[\"rank_mean\"], np.arange(len(rank_stats)), xerr=rank_stats[\"rank_std\"],\n",
    "             fmt=\"o\", capsize=3)\n",
    "plt.yticks(np.arange(len(rank_stats)), rank_stats[\"Feature\"])\n",
    "plt.gca().invert_yaxis()\n",
    "plt.xlabel(\"Average rank (smaller = more important) Â± std\"); plt.title(\"PI Rank Stability (bootstrap 15Ã—)\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(REPORT_DIR/\"D2_PI_rank_stability.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# -------------------- E. PDP + ICE + 2D interactions --------------------\n",
    "print(\"\\n[E] PDP/ICE/2D interactions\")\n",
    "preferred = [\"internet_penetration_percent\",\"power_capacity_MW_total\",\"total_data_centers\"]\n",
    "fi = pd.Series(model.feature_importances_, index=X.columns).sort_values(ascending=False)\n",
    "numeric_cols = X.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "use_feats = [c for c in preferred if c in X.columns][:3]\n",
    "for ftr in fi.index:\n",
    "    if len(use_feats) >= 3: break\n",
    "    if (ftr in numeric_cols) and (ftr not in use_feats):\n",
    "        use_feats.append(ftr)\n",
    "\n",
    "# PDP + ICE\n",
    "fig, ax = plt.subplots(1, 3, figsize=(15, 5))\n",
    "try:\n",
    "    PartialDependenceDisplay.from_estimator(model, X, use_feats, ax=ax, kind=\"both\", subsample=200)\n",
    "    fig.suptitle(\"PDP + ICE (subsample=200)\", fontsize=14)\n",
    "except Exception:\n",
    "    plt.close(fig)\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    ax.axis(\"off\"); ax.text(0.02,0.6,f\"PDP/ICE generation failed, features: {use_feats}\", fontsize=12)\n",
    "plt.tight_layout(); plt.savefig(REPORT_DIR/\"E1_PDP_ICE.png\", dpi=300); plt.close()\n",
    "\n",
    "# 2D interaction (top two numeric features)\n",
    "pair = []\n",
    "for f in fi.index:\n",
    "    if f in numeric_cols:\n",
    "        pair.append(f)\n",
    "    if len(pair) >= 2:\n",
    "        break\n",
    "\n",
    "fig = plt.figure(figsize=(6,5))\n",
    "try:\n",
    "    PartialDependenceDisplay.from_estimator(model, X, [tuple(pair)], kind=\"average\")\n",
    "    plt.title(f\"2D Interaction PDP: {pair[0]} Ã— {pair[1]}\")\n",
    "except Exception:\n",
    "    plt.close(fig)\n",
    "    fig, ax = plt.subplots(figsize=(8,3))\n",
    "    ax.axis(\"off\"); ax.text(0.02,0.6, f\"2D PDP generation failed, features: {pair}\", fontsize=12)\n",
    "plt.tight_layout(); plt.savefig(REPORT_DIR/\"E2_PDP_2D.png\", dpi=300); plt.close()\n",
    "\n",
    "# -------------------- F. Data shift: PSI (Train vs Test) --------------------\n",
    "print(\"\\n[F] PSI: Train vs Test distribution difference (top numeric features)\")\n",
    "def compute_psi(a, b, bins=10, eps=1e-9):\n",
    "    # Bins are based on 'a' (train)\n",
    "    quantiles = np.linspace(0,1,bins+1)\n",
    "    edges = np.unique(np.quantile(a, quantiles))\n",
    "    # Guard against all-equal values\n",
    "    if len(edges) < 3:\n",
    "        edges = np.linspace(min(a.min(), b.min()), max(a.max(), b.max()), bins+1)\n",
    "    a_cnt, _ = np.histogram(a, bins=edges)\n",
    "    b_cnt, _ = np.histogram(b, bins=edges)\n",
    "    a_pct = a_cnt / max(a_cnt.sum(), 1)\n",
    "    b_pct = b_cnt / max(b_cnt.sum(), 1)\n",
    "    a_pct = np.clip(a_pct, eps, None)\n",
    "    b_pct = np.clip(b_pct, eps, None)\n",
    "    psi = np.sum((a_pct - b_pct) * np.log(a_pct / b_pct))\n",
    "    return float(psi), edges\n",
    "\n",
    "top_for_psi = [f for f in fi.index if f in numeric_cols][:5]\n",
    "psi_rows = []\n",
    "for f in top_for_psi:\n",
    "    v_tr, v_te = X_train[f].values, X_test[f].values\n",
    "    val, edges = compute_psi(v_tr, v_te, bins=10)\n",
    "    psi_rows.append({\"Feature\": f, \"PSI\": val, \"Bins\": len(edges)-1})\n",
    "psi_df = pd.DataFrame(psi_rows).sort_values(\"PSI\", ascending=False)\n",
    "psi_df.to_csv(REPORT_DIR/\"F1_PSI_train_vs_test.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.barh(psi_df[\"Feature\"][::-1], psi_df[\"PSI\"][::-1])\n",
    "plt.xlabel(\"PSI (>0.2 monitor, >0.5 strong shift)\"); plt.title(\"Train vs Test PSI (top numeric features)\")\n",
    "plt.tight_layout(); plt.savefig(REPORT_DIR/\"F1_PSI_barh.png\", dpi=300); plt.close()\n",
    "\n",
    "# -------------------- Done & artifact list --------------------\n",
    "artifacts = [\n",
    "    \"GB_pred_detail.csv\",\n",
    "    \"GB_PI_coverage.csv\",\n",
    "    \"B1_CV_distribution.csv\",\n",
    "    \"B1_CV_violin.png\",\n",
    "    \"B2_validation_curve_n_estimators.png\",\n",
    "    \"C1_bucket_by_pred.csv\",\n",
    "    \"C2_bucket_by_true.csv\",\n",
    "    \"C1_MAE_by_pred_quantile.png\",\n",
    "    \"C2_MAE_by_true_quantile.png\",\n",
    "    \"D1_PI_test.csv\",\n",
    "    \"D1_PI_barh.png\",\n",
    "    \"D2_PI_rank_stability.csv\",\n",
    "    \"D2_PI_rank_stability.png\",\n",
    "    \"E1_PDP_ICE.png\",\n",
    "    \"E2_PDP_2D.png\",\n",
    "    \"F1_PSI_train_vs_test.csv\",\n",
    "    \"F1_PSI_barh.png\",\n",
    "    \"A1_pred_with_PI.png\",\n",
    "]\n",
    "print(\"\\nâœ… Enhanced analysis completed. Artifacts:\")\n",
    "for a in artifacts:\n",
    "    p = REPORT_DIR / a\n",
    "    if p.exists():\n",
    "        print(\" -\", p)\n",
    "    else:\n",
    "        print(\" - (missing)\", p)\n",
    "\n",
    "print(f\"\\nâ±ï¸ Total time: {time.time()-t0:.2f}s\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "942fbc99-ceed-4fd7-9164-1bb1fd5717b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
